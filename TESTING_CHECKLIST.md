# Testing Checklist - Deep Dive Feature

**Date:** 2026-02-16
**Status:** Ready for testing after break
**Implementation:** Simple version (BeautifulSoup + Anthropic Sonnet 4)

---

## Pre-Test Setup

### 1. Verify Dependencies
```bash
cd ~/Projects/personal-ai-agents
source venv/bin/activate
pip list | grep -E "beautifulsoup4|anthropic|requests"
```

**Expected:**
- âœ… beautifulsoup4 >= 4.14.0
- âœ… anthropic >= 0.40.0
- âœ… requests >= 2.31.0

---

### 2. Verify API Key
```bash
python3 << 'EOF'
import keyring
key = keyring.get_password('anthropic', 'api_key')
if key:
    print(f"âœ… API key found: ...{key[-4:]}")
else:
    print("âŒ API key not found")
EOF
```

**Expected:** `âœ… API key found: ...qAAA`

---

### 3. Verify Recent Briefing Exists
```bash
ls -la curator_output.txt
head -20 curator_output.txt
```

**Expected:** File exists with today's articles

---

## Test Sequence

### Test 1: Component - Article Parsing âœ“ (Already Tested)
**Purpose:** Verify flag_article.py can parse briefing

**Command:**
```bash
python3 << 'EOF'
from flag_article import parse_latest_briefing

articles = parse_latest_briefing()
if articles:
    for num in sorted(articles.keys())[:3]:
        data = articles[num]
        print(f"\n#{num}: {data['title'][:60]}...")
        print(f"   Category: {data['category']}")
        print(f"   URL: {data['url'][:50]}...")
else:
    print("âŒ No articles found")
EOF
```

**Expected Output:**
```
#1: [Article title]...
   Category: geo_major
   URL: https://...
```

**Status:** âœ… Already working (tested earlier)

---

### Test 2: Component - Article Fetching (NEW - Critical Test)
**Purpose:** Verify BeautifulSoup can extract article text

**Command:**
```bash
python3 << 'EOF'
from deep_dive import fetch_article_content

# Test with a simple article from your feed
url = "https://geopoliticalfutures.com/daily-memo-us-iran-developments-israeli-defense-industry/"

print("Testing article fetch...")
content = fetch_article_content(url)

if content:
    print(f"\nâœ… Fetch successful!")
    print(f"   Length: {len(content)} characters")
    print(f"\n   First 300 characters:")
    print(f"   {content[:300]}...")
    
    # Check if it looks like article text (not HTML garbage)
    if '<html>' in content.lower() or '<div>' in content.lower():
        print("\nâš ï¸  WARNING: Looks like HTML, not clean text")
    else:
        print("\nâœ… Content appears clean (no HTML tags)")
else:
    print("\nâŒ Fetch failed")
EOF
```

**Expected Output:**
```
Testing article fetch...
ğŸ“¡ Fetching article from https://...
âœ… BeautifulSoup: 3421 characters

âœ… Fetch successful!
   Length: 3421 characters
   
   First 300 characters:
   [Clean article text, no HTML tags]...

âœ… Content appears clean (no HTML tags)
```

**What to verify:**
- âœ… Fetches without error
- âœ… Returns reasonable length (>1000 characters)
- âœ… Text is clean (no `<html>`, `<div>`, etc.)
- âœ… Readable article content (not navigation menu garbage)

**If fails:** Check internet connection, try different URL

---

### Test 3: Component - LLM Analysis (NEW - Costs $0.15!)
**Purpose:** Verify Sonnet 4 can analyze article

**âš ï¸ WARNING: This costs ~$0.15 - only run when ready**

**Command:**
```bash
python3 << 'EOF'
from deep_dive import fetch_article_content, analyze_with_sonnet

url = "https://geopoliticalfutures.com/daily-memo-us-iran-developments-israeli-defense-industry/"

print("Fetching article...")
content = fetch_article_content(url)

if content:
    print(f"âœ… Article fetched ({len(content)} chars)\n")
    print("Analyzing with Sonnet 4 (this will take 30-60 seconds)...")
    print("ğŸ’° Cost: ~$0.15\n")
    
    analysis = analyze_with_sonnet(content, url)
    
    if analysis:
        print("âœ… Analysis complete!\n")
        print("=" * 80)
        print(analysis[:500])
        print("=" * 80)
        print(f"\nFull length: {len(analysis)} characters")
    else:
        print("âŒ Analysis failed")
else:
    print("âŒ Fetch failed, skipping analysis")
EOF
```

**Expected Output:**
```
Fetching article...
ğŸ“¡ Fetching article from https://...
âœ… BeautifulSoup: 3421 characters
âœ… Article fetched (3421 chars)

Analyzing with Sonnet 4 (this will take 30-60 seconds)...
ğŸ’° Cost: ~$0.15

ğŸ” Analyzing with Claude Sonnet 4...
âœ… Analysis complete (~600 tokens, ~$0.02)

âœ… Analysis complete!

================================================================================
# Analysis: [Article Title]

## Key Implications

â€¢ [Implication 1 with time horizons]
â€¢ [Implication 2]
...
================================================================================

Full length: 4523 characters
```

**What to verify:**
- âœ… Analysis completes without error
- âœ… Response includes all sections (Key Implications, Contrarian, Challenge, Connections)
- âœ… Quality: Specific insights, not generic fluff
- âœ… Contrarian angle: Challenges mainstream view
- âœ… Length: 500-800 words (~3000-5000 characters)

**If fails:** Check API key, Anthropic account balance

---

### Test 4: Integration - Full Deep Dive (NEW - Final Test)
**Purpose:** End-to-end test of complete workflow

**âš ï¸ WARNING: This costs ~$0.15 and saves files**

**Command:**
```bash
# Pick an interesting article from today's briefing
python flag_article.py 1 DEEP-DIVE "Testing full integration"
```

**Expected Process:**
1. Parses briefing âœ“
2. Stores interest in `interests/2026-02-16-flagged.md` âœ“
3. Triggers deep dive subprocess âœ“
4. Fetches article (BeautifulSoup) - NEW
5. Analyzes with Sonnet 4 (~30-60s) - NEW
6. Saves to `interests/deep-dives/2026-02-16-*.md` - NEW
7. Sends to Telegram (if OpenClaw running) - OPTIONAL

**Verify each step:**
```bash
# 1. Check interest was stored
cat interests/2026-02-16-flagged.md

# 2. Check deep dive file created
ls -la interests/deep-dives/

# 3. Read the analysis
cat interests/deep-dives/2026-02-16-*.md | head -100

# 4. Check Telegram (if delivered)
# Look in your Telegram for the deep dive message
```

**What to verify:**
- âœ… All files created
- âœ… Analysis quality is good (read it fully)
- âœ… No errors in console output
- âœ… Telegram delivery (optional, if OpenClaw running)

---

### Test 5: Curator Integration (Future Test)
**Purpose:** Verify next briefing applies interest boost

**Command:**
```bash
# Run curator and check for interest loading
python curator_rss_v2.py --mode=mechanical 2>&1 | grep -A 5 "Loaded.*interest"
```

**Expected Output:**
```
ğŸ“Œ Loaded 3 active interests from interests/ directory
   geo_major: 2 flagged articles (+60 total boost)
   fiscal: 1 flagged articles (+50 total boost)
```

**What to verify:**
- âœ… Curator finds and loads interests
- âœ… Counts match flagged articles
- âœ… Score modifiers correct

**Note:** This won't affect today's briefing (already run), test tomorrow morning

---

## Success Criteria

**Minimum (Must Work):**
- âœ… Article fetching extracts clean text
- âœ… Sonnet analysis completes without error
- âœ… Analysis quality is high (specific, insightful)
- âœ… Files saved correctly

**Nice to Have:**
- âœ… Telegram delivery works
- âœ… Processing time <60 seconds
- âœ… Cost under $0.20 per analysis

**Quality Bar:**
- Analysis includes contrarian perspectives
- Challenge factors are specific (not generic)
- Historical connections are relevant
- Overall: Would you read this and find it valuable?

---

## If Something Fails

### Fetch Fails
- Check internet connection
- Try different URL
- Verify URL is accessible in browser
- Check for bot-blocking (User-Agent might be blocked)

### Analysis Fails
- Check API key: `keyring.get_password('anthropic', 'api_key')`
- Check Anthropic account balance
- Verify model name: `claude-sonnet-4-20250514`
- Try shorter article (truncate to 5000 chars)

### File Save Fails
- Check directory exists: `mkdir -p interests/deep-dives`
- Check permissions: `ls -la interests/`
- Verify disk space: `df -h`

### Telegram Fails
- Check OpenClaw running: `openclaw gateway status`
- Check bot token in keyring
- Not critical - analysis still saved locally

---

## After Testing - Next Steps

**If all tests pass:**
1. Commit changes
2. Update README with test results
3. Document any issues found
4. Plan Phase 2 (xAI integration)

**If tests fail:**
1. Debug specific failure
2. Fix implementation
3. Re-test
4. Document what was wrong

---

**Ready to start? Begin with Test 2 (Article Fetching) when back from break.**
